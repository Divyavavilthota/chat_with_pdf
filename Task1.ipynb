{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoWx3twGCUOSjHjv+S9aCY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Divyavavilthota/chat_with_pdf/blob/main/Task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFjSkpD898hL",
        "outputId": "3a9539d2-2730-4f5b-dd36-8b616e2633d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.27.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.32.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.11.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2024.12.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.18.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.25.1)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.25.1)\n",
            "Requirement already satisfied: tools in /usr/local/lib/python3.10/dist-packages (0.1.9)\n",
            "Requirement already satisfied: pytils in /usr/local/lib/python3.10/dist-packages (from tools) (0.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tools) (1.17.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from tools) (5.3.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.115.6)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.41.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.10.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.42.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.2.2)\n",
            "Requirement already satisfied: frontend in /usr/local/lib/python3.10/dist-packages (0.0.3)\n",
            "Requirement already satisfied: starlette>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from frontend) (0.41.3)\n",
            "Requirement already satisfied: uvicorn>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from frontend) (0.34.0)\n",
            "Requirement already satisfied: itsdangerous>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from frontend) (2.2.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.10/dist-packages (from frontend) (24.1.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette>=0.12.0->frontend) (3.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (0.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2 sentence-transformers faiss-cpu openai\n",
        "!pip install PyMuPDF\n",
        "!pip install PyMuPDF\n",
        "!pip install tools\n",
        "!pip install fastapi\n",
        "!pip install frontend\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from whoosh.index import create_in\n",
        "from whoosh.fields import Schema, TEXT\n",
        "from whoosh.qparser import QueryParser\n",
        "from whoosh.index import open_dir\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Ensure the directory for the index exists\n",
        "index_dir = '/content/pdf_index/'\n",
        "if not os.path.exists(index_dir):\n",
        "    os.makedirs(index_dir)\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {e}\")\n",
        "    return text\n",
        "\n",
        "# Function to get the number of pages in the PDF\n",
        "def get_pdf_page_count(pdf_path):\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        return len(reader.pages)\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting page count from PDF: {e}\")\n",
        "        return 0  # Return 0 if there's an error\n",
        "\n",
        "# Function to search the text in the index\n",
        "def search_text(query_string):\n",
        "    ix = open_dir(index_dir)  # Use the directory with the index\n",
        "    with ix.searcher() as searcher:\n",
        "        query = QueryParser(\"content\", ix.schema).parse(query_string)\n",
        "        results = searcher.search(query, limit=5)  # Limit results to top 5\n",
        "        response = []\n",
        "        for result in results:\n",
        "            response.append({\n",
        "                'content': result['content'],\n",
        "                'score': result.score,\n",
        "                'highlights': result.highlights(\"content\")  # Show highlighted match\n",
        "            })\n",
        "        return response\n",
        "\n",
        "# Example usage\n",
        "pdf_path = \"/content/sample.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Check if text is extracted\n",
        "print(\"Extracted Text: \", extracted_text[:500])  # Print first 500 characters to verify\n",
        "\n",
        "# Get the number of pages in the PDF\n",
        "page_count = get_pdf_page_count(pdf_path)\n",
        "print(f\"Number of pages in the PDF: {page_count}\")\n",
        "\n",
        "# Create an index\n",
        "schema = Schema(content=TEXT(stored=True))\n",
        "ix = create_in(index_dir, schema)\n",
        "writer = ix.writer()\n",
        "\n",
        "# Check if the document is being indexed properly\n",
        "if extracted_text.strip():  # Only index if there's text to index\n",
        "    print(\"Indexing document: \", extracted_text[:500])  # Print first 500 characters\n",
        "    writer.add_document(content=extracted_text)\n",
        "    writer.commit()\n",
        "else:\n",
        "    print(\"No text extracted from PDF, skipping indexing.\")\n",
        "\n",
        "# Query the index with a more specific or broader query (e.g., 'What is the primary objective of the study?')\n",
        "query = \"what is abstract?\"  # Update with a relevant question\n",
        "response = search_text(query)\n",
        "\n",
        "# Output the response\n",
        "if response:\n",
        "    print(\"Response:\")\n",
        "    for idx, res in enumerate(response, 1):\n",
        "        print(f\"Result {idx}:\")\n",
        "        print(f\"Score: {res['score']}\")\n",
        "        print(f\"Content: {res['content'][10:]}...\")  # Display first 500 characters of result\n",
        "        print(f\"Highlights: {res['highlights']}\")\n",
        "else:\n",
        "    print(\"No results found for the query.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tfd2hp6fFKnF",
        "outputId": "bd070141-bc73-4b68-c62f-969ff21e9f7f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Text:  Estimating\n",
            "Countries\n",
            "with\n",
            "Similar\n",
            "Maternal\n",
            "Mortality\n",
            "Rate\n",
            "using\n",
            "Cluster\n",
            "Analysis\n",
            "and\n",
            "Pairing\n",
            "Countries\n",
            "with\n",
            "Identical\n",
            "MMR\n",
            "Dr.\n",
            "S.\n",
            "Nandini\n",
            "Associate\n",
            "Professor\n",
            "in\n",
            "Zoology\n",
            "Quaid-E-Millath\n",
            "Government\n",
            "College\n",
            "for\n",
            "Women,\n",
            "Anna\n",
            "Salai,\n",
            "Chennai\n",
            "-\n",
            "600\n",
            "002\n",
            "s.nandini0507@gmail.com\n",
            "Sanjjushri\n",
            "Varshini\n",
            "R\n",
            "sanjjushrivarshini@gmail.com\n",
            "Abstract:\n",
            "In\n",
            "the\n",
            "evolving\n",
            "world,\n",
            "we\n",
            "require\n",
            "more\n",
            "additionally\n",
            "the\n",
            "young\n",
            "era\n",
            "to\n",
            "flourish\n",
            "and\n",
            "evolve\n",
            "into\n",
            "developed\n",
            "land.\n",
            "Most\n",
            "of\n",
            "the\n",
            "population\n",
            "all\n",
            "around\n",
            "the\n",
            "world\n",
            "are\n",
            "unaware\n",
            "of\n",
            "th\n",
            "Number of pages in the PDF: 14\n",
            "Indexing document:  Estimating\n",
            "Countries\n",
            "with\n",
            "Similar\n",
            "Maternal\n",
            "Mortality\n",
            "Rate\n",
            "using\n",
            "Cluster\n",
            "Analysis\n",
            "and\n",
            "Pairing\n",
            "Countries\n",
            "with\n",
            "Identical\n",
            "MMR\n",
            "Dr.\n",
            "S.\n",
            "Nandini\n",
            "Associate\n",
            "Professor\n",
            "in\n",
            "Zoology\n",
            "Quaid-E-Millath\n",
            "Government\n",
            "College\n",
            "for\n",
            "Women,\n",
            "Anna\n",
            "Salai,\n",
            "Chennai\n",
            "-\n",
            "600\n",
            "002\n",
            "s.nandini0507@gmail.com\n",
            "Sanjjushri\n",
            "Varshini\n",
            "R\n",
            "sanjjushrivarshini@gmail.com\n",
            "Abstract:\n",
            "In\n",
            "the\n",
            "evolving\n",
            "world,\n",
            "we\n",
            "require\n",
            "more\n",
            "additionally\n",
            "the\n",
            "young\n",
            "era\n",
            "to\n",
            "flourish\n",
            "and\n",
            "evolve\n",
            "into\n",
            "developed\n",
            "land.\n",
            "Most\n",
            "of\n",
            "the\n",
            "population\n",
            "all\n",
            "around\n",
            "the\n",
            "world\n",
            "are\n",
            "unaware\n",
            "of\n",
            "th\n",
            "No results found for the query.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from whoosh.index import create_in\n",
        "from whoosh.fields import Schema, TEXT\n",
        "from whoosh.qparser import QueryParser\n",
        "from whoosh.index import open_dir\n",
        "from PyPDF2 import PdfReader\n",
        "import re\n",
        "\n",
        "# Ensure the directory for the index exists\n",
        "index_dir = '/content/pdf_index/'\n",
        "if not os.path.exists(index_dir):\n",
        "    os.makedirs(index_dir)\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {e}\")\n",
        "    return text\n",
        "\n",
        "# Function to get the number of pages in the PDF\n",
        "def get_pdf_page_count(pdf_path):\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        return len(reader.pages)\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting page count from PDF: {e}\")\n",
        "        return 0  # Return 0 if there's an error\n",
        "\n",
        "# Function to extract the abstract from the document\n",
        "def extract_abstract(text):\n",
        "    # Look for text that typically starts an abstract section with more flexibility\n",
        "    match = re.search(r\"(?i)(abstract|summary)[^:]*[:\\s]*\\n*(.*?)\\n+(?:Introduction|1\\.)\", text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(2).strip()\n",
        "    else:\n",
        "        return \"Abstract not found in the document.\"\n",
        "\n",
        "# Function to search the text in the index\n",
        "def search_text(query_string):\n",
        "    ix = open_dir(index_dir)  # Use the directory with the index\n",
        "    with ix.searcher() as searcher:\n",
        "        query = QueryParser(\"content\", ix.schema).parse(query_string)\n",
        "        results = searcher.search(query, limit=5)  # Limit results to top 5\n",
        "        response = []\n",
        "        for result in results:\n",
        "            response.append({\n",
        "                'content': result['content'],\n",
        "                'score': result.score,\n",
        "                'highlights': result.highlights(\"content\")  # Show highlighted match\n",
        "            })\n",
        "        return response\n",
        "\n",
        "# Function to answer specific questions\n",
        "def answer_question(query_string, pdf_path, extracted_text):\n",
        "    # Check if the query is about the number of pages in the PDF\n",
        "    if \"how many pages\" in query_string.lower():\n",
        "        page_count = get_pdf_page_count(pdf_path)\n",
        "        return f\"The number of pages in the document is: {page_count}\"\n",
        "\n",
        "    # Check if the query is about the abstract\n",
        "    if \"abstract\" in query_string.lower():\n",
        "        abstract = extract_abstract(extracted_text)\n",
        "        return f\"Abstract: {abstract}\"\n",
        "\n",
        "    # If the question is not about the page count or abstract, search the text in the index\n",
        "    response = search_text(query_string)\n",
        "    if response:\n",
        "        return f\"Found the following related information:\\n\" + \"\\n\".join([res['content'][:500] for res in response])  # Show first 500 characters\n",
        "    else:\n",
        "        return \"No relevant information found for your query.\"\n",
        "\n",
        "# Example usage\n",
        "pdf_path = \"sample.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Check if text is extracted\n",
        "if extracted_text.strip():  # Only index if there's text to index\n",
        "    # Create an index and add document\n",
        "    schema = Schema(content=TEXT(stored=True))\n",
        "    ix = create_in(index_dir, schema)\n",
        "    writer = ix.writer()\n",
        "    writer.add_document(content=extracted_text)\n",
        "    writer.commit()\n",
        "else:\n",
        "    print(\"No text extracted from PDF, skipping indexing.\")\n",
        "\n",
        "# Ask a question related to the document\n",
        "query = \"What is the abstract and keywords?\"\n",
        "response = answer_question(query, pdf_path, extracted_text)\n",
        "\n",
        "# Output the response\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KSLcjswJhGE",
        "outputId": "34efbd4f-716f-4226-ed58-d711e10bb4e9"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abstract: In\n",
            "the\n",
            "evolving\n",
            "world,\n",
            "we\n",
            "require\n",
            "more\n",
            "additionally\n",
            "the\n",
            "young\n",
            "era\n",
            "to\n",
            "flourish\n",
            "and\n",
            "evolve\n",
            "into\n",
            "developed\n",
            "land.\n",
            "Most\n",
            "of\n",
            "the\n",
            "population\n",
            "all\n",
            "around\n",
            "the\n",
            "world\n",
            "are\n",
            "unaware\n",
            "of\n",
            "the\n",
            "complications\n",
            "involved\n",
            "in\n",
            "the\n",
            "routine\n",
            "they\n",
            "follow\n",
            "while\n",
            "they\n",
            "are\n",
            "pregnant\n",
            "and\n",
            "how\n",
            "hospital\n",
            "facilities\n",
            "affect\n",
            "maternal\n",
            "health.\n",
            "Maternal\n",
            "Mortality\n",
            "is\n",
            "the\n",
            "death\n",
            "of\n",
            "a\n",
            "pregnant\n",
            "woman\n",
            "due\n",
            "to\n",
            "intricacies\n",
            "correlated\n",
            "to\n",
            "pregnancy,\n",
            "underlying\n",
            "circumstances\n",
            "exacerbated\n",
            "by\n",
            "the\n",
            "pregnancy\n",
            "or\n",
            "management\n",
            "of\n",
            "these\n",
            "situations.\n",
            "It\n",
            "is\n",
            "crucial\n",
            "to\n",
            "consider\n",
            "the\n",
            "Maternal\n",
            "Mortality\n",
            "Rate\n",
            "(MMR)\n",
            "in\n",
            "diverse\n",
            "locations\n",
            "and\n",
            "determine\n",
            "which\n",
            "human\n",
            "routines\n",
            "and\n",
            "hospital\n",
            "facilities\n",
            "diminish\n",
            "the\n",
            "Maternal\n",
            "Mortality\n",
            "Rate\n",
            "(MMR).\n",
            "This\n",
            "research\n",
            "aims\n",
            "to\n",
            "examine\n",
            "and\n",
            "discover\n",
            "the\n",
            "countries\n",
            "which\n",
            "are\n",
            "keeping\n",
            "more\n",
            "lavish\n",
            "threats\n",
            "of\n",
            "MMR\n",
            "and\n",
            "countries\n",
            "alike\n",
            "in\n",
            "MMR\n",
            "encountered.\n",
            "Data\n",
            "is\n",
            "examined\n",
            "and\n",
            "collected\n",
            "for\n",
            "various\n",
            "countries,\n",
            "data\n",
            "consists\n",
            "of\n",
            "the\n",
            "earlier\n",
            "years'\n",
            "observation.\n",
            "From\n",
            "the\n",
            "perspective\n",
            "of\n",
            "Machine\n",
            "Learning,\n",
            "Unsupervised\n",
            "Machine\n",
            "Learning\n",
            "is\n",
            "implemented\n",
            "to\n",
            "perform\n",
            "Cluster\n",
            "Analysis.\n",
            "Therefore\n",
            "the\n",
            "pairs\n",
            "of\n",
            "countries\n",
            "with\n",
            "similar\n",
            "MMR\n",
            "as\n",
            "well\n",
            "as\n",
            "the\n",
            "extreme\n",
            "opposite\n",
            "pair\n",
            "concerning\n",
            "the\n",
            "MMR\n",
            "are\n",
            "found.\n",
            "Keywords:\n",
            "Machine\n",
            "Learning,\n",
            "Unsupervised\n",
            "Machine\n",
            "Learning,\n",
            "Cluster\n",
            "Analysis,\n",
            "Maternal\n",
            "Mortality\n",
            "Rate\n",
            "(MMR),\n",
            "Similar\n",
            "countries\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "J-X4BszKKmut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "print(\"Extracted Text: \", extracted_text[:500])  # Print the first 500 characters to verify\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14zN1HXVGff6",
        "outputId": "f0d89b4b-8d3b-4718-e928-a097a0cdd9b3"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Text:  Estimating\n",
            "Countries\n",
            "with\n",
            "Similar\n",
            "Maternal\n",
            "Mortality\n",
            "Rate\n",
            "using\n",
            "Cluster\n",
            "Analysis\n",
            "and\n",
            "Pairing\n",
            "Countries\n",
            "with\n",
            "Identical\n",
            "MMR\n",
            "Dr.\n",
            "S.\n",
            "Nandini\n",
            "Associate\n",
            "Professor\n",
            "in\n",
            "Zoology\n",
            "Quaid-E-Millath\n",
            "Government\n",
            "College\n",
            "for\n",
            "Women,\n",
            "Anna\n",
            "Salai,\n",
            "Chennai\n",
            "-\n",
            "600\n",
            "002\n",
            "s.nandini0507@gmail.com\n",
            "Sanjjushri\n",
            "Varshini\n",
            "R\n",
            "sanjjushrivarshini@gmail.com\n",
            "Abstract:\n",
            "In\n",
            "the\n",
            "evolving\n",
            "world,\n",
            "we\n",
            "require\n",
            "more\n",
            "additionally\n",
            "the\n",
            "young\n",
            "era\n",
            "to\n",
            "flourish\n",
            "and\n",
            "evolve\n",
            "into\n",
            "developed\n",
            "land.\n",
            "Most\n",
            "of\n",
            "the\n",
            "population\n",
            "all\n",
            "around\n",
            "the\n",
            "world\n",
            "are\n",
            "unaware\n",
            "of\n",
            "th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from whoosh.index import create_in\n",
        "from whoosh.fields import Schema, TEXT\n",
        "from whoosh.qparser import QueryParser\n",
        "from whoosh.index import open_dir\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Ensure the directory for the index exists\n",
        "index_dir = '/content/pdf_index/'\n",
        "if not os.path.exists(index_dir):\n",
        "    os.makedirs(index_dir)\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {e}\")\n",
        "    return text\n",
        "\n",
        "# Function to search the text in the index\n",
        "def search_text(query_string):\n",
        "    ix = open_dir(index_dir)  # Use the directory with the index\n",
        "    with ix.searcher() as searcher:\n",
        "        query = QueryParser(\"content\", ix.schema).parse(query_string)\n",
        "        results = searcher.search(query)\n",
        "        return [result['content'] for result in results]\n",
        "\n",
        "# Example usage\n",
        "pdf_path = \"/content/sample.pdf\"\n",
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Check if text is extracted\n",
        "print(\"Extracted Text: \", extracted_text[:500])  # Print first 500 characters to verify\n",
        "\n",
        "# Create an index\n",
        "schema = Schema(content=TEXT(stored=True))\n",
        "ix = create_in(index_dir, schema)\n",
        "writer = ix.writer()\n",
        "\n",
        "# Check if the document is being indexed properly\n",
        "if extracted_text.strip():  # Only index if there's text to index\n",
        "    print(\"Indexing document: \", extracted_text[:500])  # Print first 500 characters\n",
        "    writer.add_document(content=extracted_text)\n",
        "    writer.commit()\n",
        "else:\n",
        "    print(\"No text extracted from PDF, skipping indexing.\")\n",
        "\n",
        "# Query the index with a broader query\n",
        "query = \"how many pages are there?\"\n",
        "response = search_text(query)\n",
        "\n",
        "# Output the response\n",
        "print(\"Response:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17tGq2WYG_L6",
        "outputId": "b5952c22-f964-4773-9a9b-402a4f06e6e9"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Text:  Estimating\n",
            "Countries\n",
            "with\n",
            "Similar\n",
            "Maternal\n",
            "Mortality\n",
            "Rate\n",
            "using\n",
            "Cluster\n",
            "Analysis\n",
            "and\n",
            "Pairing\n",
            "Countries\n",
            "with\n",
            "Identical\n",
            "MMR\n",
            "Dr.\n",
            "S.\n",
            "Nandini\n",
            "Associate\n",
            "Professor\n",
            "in\n",
            "Zoology\n",
            "Quaid-E-Millath\n",
            "Government\n",
            "College\n",
            "for\n",
            "Women,\n",
            "Anna\n",
            "Salai,\n",
            "Chennai\n",
            "-\n",
            "600\n",
            "002\n",
            "s.nandini0507@gmail.com\n",
            "Sanjjushri\n",
            "Varshini\n",
            "R\n",
            "sanjjushrivarshini@gmail.com\n",
            "Abstract:\n",
            "In\n",
            "the\n",
            "evolving\n",
            "world,\n",
            "we\n",
            "require\n",
            "more\n",
            "additionally\n",
            "the\n",
            "young\n",
            "era\n",
            "to\n",
            "flourish\n",
            "and\n",
            "evolve\n",
            "into\n",
            "developed\n",
            "land.\n",
            "Most\n",
            "of\n",
            "the\n",
            "population\n",
            "all\n",
            "around\n",
            "the\n",
            "world\n",
            "are\n",
            "unaware\n",
            "of\n",
            "th\n",
            "Indexing document:  Estimating\n",
            "Countries\n",
            "with\n",
            "Similar\n",
            "Maternal\n",
            "Mortality\n",
            "Rate\n",
            "using\n",
            "Cluster\n",
            "Analysis\n",
            "and\n",
            "Pairing\n",
            "Countries\n",
            "with\n",
            "Identical\n",
            "MMR\n",
            "Dr.\n",
            "S.\n",
            "Nandini\n",
            "Associate\n",
            "Professor\n",
            "in\n",
            "Zoology\n",
            "Quaid-E-Millath\n",
            "Government\n",
            "College\n",
            "for\n",
            "Women,\n",
            "Anna\n",
            "Salai,\n",
            "Chennai\n",
            "-\n",
            "600\n",
            "002\n",
            "s.nandini0507@gmail.com\n",
            "Sanjjushri\n",
            "Varshini\n",
            "R\n",
            "sanjjushrivarshini@gmail.com\n",
            "Abstract:\n",
            "In\n",
            "the\n",
            "evolving\n",
            "world,\n",
            "we\n",
            "require\n",
            "more\n",
            "additionally\n",
            "the\n",
            "young\n",
            "era\n",
            "to\n",
            "flourish\n",
            "and\n",
            "evolve\n",
            "into\n",
            "developed\n",
            "land.\n",
            "Most\n",
            "of\n",
            "the\n",
            "population\n",
            "all\n",
            "around\n",
            "the\n",
            "world\n",
            "are\n",
            "unaware\n",
            "of\n",
            "th\n",
            "Response: []\n"
          ]
        }
      ]
    }
  ]
}